{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCZmQKLpBtFQ"
      },
      "source": [
        "## DQN for continuous action spaces: Normalized Advantage Function (NAF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5s0R3qxBnTN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt-get update && apt-get install -y xvfb\n",
        "!pip install swig\n",
        "!pip install gym[box2d]==0.23.1 pytorch-lightning==1.6.0 pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anaconda3\\Lib\\site-packages\\lightning_utilities\\core\\imports.py:14: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n",
            "d:\\anaconda3\\Lib\\site-packages\\pkg_resources\\__init__.py:3117: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "d:\\anaconda3\\Lib\\site-packages\\pkg_resources\\__init__.py:3117: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "d:\\anaconda3\\Lib\\site-packages\\pkg_resources\\__init__.py:3117: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "d:\\anaconda3\\Lib\\site-packages\\pkg_resources\\__init__.py:3117: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('zope')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "d:\\anaconda3\\Lib\\site-packages\\botocore\\utils.py:15: DeprecationWarning: 'cgi' is deprecated and slated for removal in Python 3.13\n",
            "  import cgi\n",
            "d:\\anaconda3\\Lib\\site-packages\\botocore\\httpsession.py:41: DeprecationWarning: 'urllib3.contrib.pyopenssl' module is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680\n",
            "  from urllib3.contrib.pyopenssl import orig_util_SSLContext as SSLContext\n",
            "d:\\anaconda3\\Lib\\site-packages\\lightning_fabric\\__init__.py:41: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "d:\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import gym\n",
        "import random\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos1/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "3.0\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "action_dims_1=2\n",
        "print(action_dims_1 + 1)\n",
        "print(action_dims_1 * (action_dims_1 + 1) / 2)\n",
        "print(int(action_dims_1 * (action_dims_1 + 1) / 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMKrYHMnFISO"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pGAOZKhbBu_z"
      },
      "outputs": [],
      "source": [
        "class NafDQN(nn.Module):\n",
        "    \n",
        "  def __init__(self, hidden_size, obs_size, action_dims, max_action):\n",
        "    super().__init__()\n",
        "    self.action_dims = action_dims\n",
        "    print(f\"This is the action_dims {self.action_dims}\")\n",
        "    self.max_action = torch.from_numpy(max_action).to(device)\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(obs_size, hidden_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_size, hidden_size),\n",
        "      nn.ReLU(),   \n",
        "    )\n",
        "    self.linear_mu = nn.Linear(hidden_size, action_dims)#see the change here, this is not number of actions, number of actions are infinite\n",
        "    self.linear_value = nn.Linear(hidden_size, 1)\n",
        "    self.linear_matrix = nn.Linear(hidden_size, int(action_dims * (action_dims + 1) / 2))\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def mu(self, x):\n",
        "    x = self.net(x)\n",
        "    x = self.linear_mu(x)\n",
        "    x = torch.tanh(x) * self.max_action\n",
        "    return x\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def value(self, x):\n",
        "    x = self.net(x)\n",
        "    x = self.linear_value(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, a):\n",
        "    x = self.net(x)\n",
        "    mu = torch.tanh(self.linear_mu(x)) * self.max_action\n",
        "    value = self.linear_value(x)\n",
        "    matrix = torch.tanh(self.linear_matrix(x))\n",
        "#     But as you can imagine right now, this matrix variable is actually a vector with values because the\n",
        "\n",
        "# entries for that matrix have been produced by a linear layer.\n",
        "\n",
        "# Now it's time for us to put these entries in an actual matrix.\n",
        "\n",
        "# So what we are going to do is create an empty matrix that will called L because this is a lower triangular\n",
        "\n",
        "# matrix and we will fill this matrix with the value zero.\n",
        "\n",
        "# And how many entries should this matrix have?\n",
        "\n",
        "# Well, it should have action dims by action dimes entries.\n",
        "#[action_dims*actions_dims]\n",
        "\n",
        "# But remember that in the forward pass, we are working with a bunch of observations, which means that\n",
        "\n",
        "# instead of working with a single state, we might be working with eight, 16, 32, etc. So actually\n",
        "\n",
        "# our matrix should hold batch_size * action_dims * action_dims\n",
        "\n",
        "# That is one matrix of this size for each of the states in this batch.\n",
        "\n",
        "# And to do it, we are going to take the states Tensor X and find its butt size, which is in the 0th\n",
        "\n",
        "# position of its shape property.\n",
        "\n",
        "# And then the other dimensions should be the action dims, the number of dimensions of each action.\n",
        "    \n",
        "    L = torch.zeros((x.shape[0], self.action_dims, self.action_dims)).to(device)\n",
        "    tril_indices = torch.tril_indices(row=self.action_dims, col=self.action_dims, offset=0).to(device)\n",
        "    #tril_indices will give us the index values to index a lower triangular matrix\n",
        "\n",
        "    L[:, tril_indices[0], tril_indices[1]] = matrix\n",
        "    L.diagonal(dim1=1,dim2=2).exp_()#This will ensure that the values in the diagonal are positive\n",
        "    #Though I can't understand what the arguments dim1=1 and dim2=2 are doing\n",
        "    #oh, maybe it's denoting the dimension of which matrices\n",
        "    # dimension 0 denotes just batch size, the actual matrices dimension inside the 3-D matrix is in dimension 1 and dimension 2\n",
        "    P = L * L.transpose(2, 1)\n",
        "    #and here, we want to take transpose of matrices\n",
        "    u_mu = (a-mu).unsqueeze(dim=1)\n",
        "    u_mu_t = u_mu.transpose(1, 2)\n",
        "    #because dimension 0 is the batch size\n",
        "    \n",
        "    adv = - 1/2 * u_mu @ P @ u_mu_t\n",
        "    #after this operation, the dimension is [[[]]] but we need [[],[],]\n",
        "    adv = adv.squeeze(dim=-1)#This line of code removes the extra dimension\n",
        "    return value + adv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hziRGjJ9Pkv1"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5B223HLzBvCx"
      },
      "outputs": [],
      "source": [
        "def noisy_policy(state, env, net, epsilon=0.0):\n",
        "  state = torch.tensor([state]).to(device)#first, we are going to take a state, and create a tensor from it, we'll put it inside a list so that pytorch knows that we are working with a batch with a single item and then we'll make sure that it's on the right device\n",
        "  amin = torch.from_numpy(env.action_space.low).to(device)#minimum value that each dimension of the action can take(this will be an array, try to imagine and understand)\n",
        "  amax = torch.from_numpy(env.action_space.high).to(device)#maximum value that each dimension of the action can take(this will be an array, try to imagine and understand)\n",
        "  mu = net.mu(state)#this will give us the action in the state that our neural estimates has the highest q-value\n",
        "  mu = mu + torch.normal(0, epsilon, mu.size(), device=device)#we are adding some noise to this action to promote exploration\n",
        "  action = mu.clamp(amin, amax)\n",
        "  action = action.squeeze().cpu().numpy()\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yeo3s-QPnZH"
      },
      "source": [
        "#### Create the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fw-77TRyBvHz"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  \n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YtGko6LVQaJz"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihkyoL5WQgGm"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9XQlZy9J-vjt"
      },
      "outputs": [],
      "source": [
        "#we will apply the same action several times to keep the agent a bit shaky\n",
        "#especially in robotics task, where we want out actions to be precise and consistent, if we modify\n",
        "#our actions too quickly in the environment, our robot can end up having unpredictable behaviour\n",
        "#to mitigate this problem, we create a new class\n",
        "class RepeatActionWrapper(gym.Wrapper):#that extends over gym.Wrapper\n",
        "  #and apply the same action on the following states\n",
        "  def __init__(self, env, n):\n",
        "    super().__init__(env)\n",
        "    self.env = env\n",
        "    self.n = n\n",
        "      \n",
        "  def step(self, action):\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    for _ in range(self.n):\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    return next_state, total_reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xZ3h8CCOQjGL"
      },
      "outputs": [],
      "source": [
        "def create_environment(name):\n",
        "  env = gym.make(name)\n",
        "  env = RecordVideo(env, video_folder='./videos1', episode_trigger=lambda x: x % 50 == 0)\n",
        "  env = RepeatActionWrapper(env, n=8)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8fKGgFzQ4EX"
      },
      "source": [
        "#### Update the target network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-q-OJaPnBvKf"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8de_OtyR1oJ"
      },
      "source": [
        "#### Create the Deep Q-Learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N-tOW8KgBvNZ"
      },
      "outputs": [],
      "source": [
        "class NAFDeepQLearning(LightningModule):\n",
        "                             \n",
        "  def __init__(self, env_name, policy=noisy_policy, capacity=100_000, \n",
        "               batch_size=256, lr=1e-4, hidden_size=512, gamma=0.99, \n",
        "               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=2.0, eps_end=0.2, \n",
        "               eps_last_episode=1_000, samples_per_epoch=1_000, tau=0.01):\n",
        "\n",
        "    super().__init__()\n",
        "    self.env = create_environment(env_name)\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[0]\n",
        "    action_dims = self.env.action_space.shape[0]\n",
        "    max_action = self.env.action_space.high\n",
        "\n",
        "    self.q_net = NafDQN(hidden_size, obs_size, action_dims, max_action).to(device)\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "    self.policy = policy\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()#save the hyperaparameters so that they are accessible\n",
        "    #everywhere in the class\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode(epsilon=self.hparams.eps_start)\n",
        "  \n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None, epsilon=0.):\n",
        "    obs = self.env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      if policy:\n",
        "        action = policy(obs, self.env, self.q_net, epsilon=epsilon)\n",
        "      else:\n",
        "        action = self.env.action_space.sample()\n",
        "        \n",
        "      next_obs, reward, done, info = self.env.step(action)\n",
        "      exp = (obs, action, reward, done, next_obs)\n",
        "      self.buffer.append(exp)\n",
        "      obs = next_obs\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.q_net.mu(x)\n",
        "    return output\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer]\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    action_values = self.q_net(states, actions)\n",
        "\n",
        "    next_state_values = self.target_q_net.value(next_states)\n",
        "    next_state_values[dones] = 0.0\n",
        "    \n",
        "    target = rewards + self.hparams.gamma * next_state_values\n",
        "\n",
        "    loss = self.hparams.loss_fn(action_values, target)\n",
        "    self.log('episode/MSE Loss', loss)\n",
        "    return loss\n",
        "\n",
        "  def on_train_epoch_end(self):\n",
        "\n",
        "    epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "    )\n",
        "\n",
        "    self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "    \n",
        "    polyak_average(self.q_net, self.target_q_net, tau=self.hparams.tau)\n",
        "    \n",
        "    self.log(\"episode/Return\", self.env.return_queue[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCusNrF-SPHP"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zOhCyJgTBvQR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 2).\n",
              "Contents of stderr:\n",
              "2024-07-03 12:21:09.657781: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
              "2024-07-03 12:21:10.737411: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
              "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
              "                   [--host ADDR] [--bind_all] [--port PORT]\n",
              "                   [--reuse_port BOOL] [--load_fast {false,auto,true}]\n",
              "                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
              "                   [--grpc_creds_type {local,ssl,ssl_dev}]\n",
              "                   [--grpc_data_provider PORT] [--purge_orphaned_data BOOL]\n",
              "                   [--db URI] [--db_import] [--inspect] [--version_tb]\n",
              "                   [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n",
              "                   [--window_title TEXT] [--max_reload_threads COUNT]\n",
              "                   [--reload_interval SECONDS] [--reload_task TYPE]\n",
              "                   [--reload_multifile BOOL]\n",
              "                   [--reload_multifile_inactive_secs SECONDS]\n",
              "                   [--generic_data TYPE]\n",
              "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
              "                   [--detect_file_replacement BOOL]\n",
              "                   {serve} ...\n",
              "tensorboard: error: argument {serve}: invalid choice: 'Learning/advanced_rl_dqn_to_sac_complete-main/lightning_logs' (choose from 'serve')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Start tensorboard.\n",
        "# !rm -r /content/lightning_logs/\n",
        "# !rm -r /content/videos/\n",
        "# %load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir D:/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/lightning_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-fecCQPSVD6"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yY3-mV12BvTK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\Reinforcement Learning\\advanced_rl_dqn_to_sac_complete-main\\videos1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the action_dims 2\n",
            "0 samples in experience buffer. Filling...\n",
            "14 samples in experience buffer. Filling...\n",
            "29 samples in experience buffer. Filling...\n",
            "41 samples in experience buffer. Filling...\n",
            "63 samples in experience buffer. Filling...\n",
            "71 samples in experience buffer. Filling...\n",
            "85 samples in experience buffer. Filling...\n",
            "99 samples in experience buffer. Filling...\n",
            "120 samples in experience buffer. Filling...\n",
            "137 samples in experience buffer. Filling...\n",
            "147 samples in experience buffer. Filling...\n",
            "160 samples in experience buffer. Filling...\n",
            "170 samples in experience buffer. Filling...\n",
            "182 samples in experience buffer. Filling...\n",
            "192 samples in experience buffer. Filling...\n",
            "201 samples in experience buffer. Filling...\n",
            "212 samples in experience buffer. Filling...\n",
            "223 samples in experience buffer. Filling...\n",
            "231 samples in experience buffer. Filling...\n",
            "244 samples in experience buffer. Filling...\n",
            "255 samples in experience buffer. Filling...\n",
            "265 samples in experience buffer. Filling...\n",
            "279 samples in experience buffer. Filling...\n",
            "294 samples in experience buffer. Filling...\n",
            "304 samples in experience buffer. Filling...\n",
            "313 samples in experience buffer. Filling...\n",
            "326 samples in experience buffer. Filling...\n",
            "337 samples in experience buffer. Filling...\n",
            "352 samples in experience buffer. Filling...\n",
            "364 samples in experience buffer. Filling...\n",
            "378 samples in experience buffer. Filling...\n",
            "390 samples in experience buffer. Filling...\n",
            "404 samples in experience buffer. Filling...\n",
            "412 samples in experience buffer. Filling...\n",
            "431 samples in experience buffer. Filling...\n",
            "450 samples in experience buffer. Filling...\n",
            "459 samples in experience buffer. Filling...\n",
            "471 samples in experience buffer. Filling...\n",
            "480 samples in experience buffer. Filling...\n",
            "491 samples in experience buffer. Filling...\n",
            "502 samples in experience buffer. Filling...\n",
            "513 samples in experience buffer. Filling...\n",
            "521 samples in experience buffer. Filling...\n",
            "539 samples in experience buffer. Filling...\n",
            "558 samples in experience buffer. Filling...\n",
            "570 samples in experience buffer. Filling...\n",
            "585 samples in experience buffer. Filling...\n",
            "596 samples in experience buffer. Filling...\n",
            "608 samples in experience buffer. Filling...\n",
            "621 samples in experience buffer. Filling...\n",
            "633 samples in experience buffer. Filling...\n",
            "642 samples in experience buffer. Filling...\n",
            "653 samples in experience buffer. Filling...\n",
            "669 samples in experience buffer. Filling...\n",
            "680 samples in experience buffer. Filling...\n",
            "695 samples in experience buffer. Filling...\n",
            "712 samples in experience buffer. Filling...\n",
            "726 samples in experience buffer. Filling...\n",
            "736 samples in experience buffer. Filling...\n",
            "747 samples in experience buffer. Filling...\n",
            "759 samples in experience buffer. Filling...\n",
            "773 samples in experience buffer. Filling...\n",
            "797 samples in experience buffer. Filling...\n",
            "807 samples in experience buffer. Filling...\n",
            "825 samples in experience buffer. Filling...\n",
            "833 samples in experience buffer. Filling...\n",
            "846 samples in experience buffer. Filling...\n",
            "860 samples in experience buffer. Filling...\n",
            "872 samples in experience buffer. Filling...\n",
            "882 samples in experience buffer. Filling...\n",
            "892 samples in experience buffer. Filling...\n",
            "905 samples in experience buffer. Filling...\n",
            "915 samples in experience buffer. Filling...\n",
            "928 samples in experience buffer. Filling...\n",
            "941 samples in experience buffer. Filling...\n",
            "956 samples in experience buffer. Filling...\n",
            "971 samples in experience buffer. Filling...\n",
            "984 samples in experience buffer. Filling...\n",
            "995 samples in experience buffer. Filling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "d:\\anaconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name         | Type   | Params | Mode \n",
            "------------------------------------------------\n",
            "0 | q_net        | NafDQN | 270 K  | train\n",
            "1 | target_q_net | NafDQN | 270 K  | train\n",
            "------------------------------------------------\n",
            "540 K     Trainable params\n",
            "0         Non-trainable params\n",
            "540 K     Total params\n",
            "2.163     Total estimated model params size (MB)\n",
            "d:\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "d:\\anaconda3\\Lib\\site-packages\\ipywidgets\\widgets\\widget.py:528: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
            "  self.comm = Comm(**args)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b444ebff329426d8a2ee50cc6deacd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\haadi\\AppData\\Local\\Temp\\ipykernel_21896\\1522879196.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
            "  state = torch.tensor([state]).to(device)#first, we are going to take a state, and create a tensor from it, we'll put it inside a list so that pytorch knows that we are working with a batch with a single item and then we'll make sure that it's on the right device\n",
            "`Trainer.fit` stopped: `max_epochs=10000` reached.\n"
          ]
        }
      ],
      "source": [
        "algo = NAFDeepQLearning('LunarLanderContinuous-v2')\n",
        "\n",
        "trainer = Trainer(\n",
        "    # gpus=num_gpus, \n",
        "    max_epochs=10_000\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aURap6K8SXLh"
      },
      "source": [
        "#### Check the resulting policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0q9_nEOBvV3"
      },
      "outputs": [],
      "source": [
        "display_video(episode=4300)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
