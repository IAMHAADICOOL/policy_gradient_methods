{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GcEjHjuwnXW"
      },
      "source": [
        "## Hindsight Experience Replay (HER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29QddByhtuOm"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    xvfb \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common \\\n",
        "    patchelf\n",
        "\n",
        "!pip install \\\n",
        "    free-mujoco-py \\\n",
        "    gym==0.21 \\\n",
        "    pytorch-lightning==1.6.0 \\\n",
        "    pyvirtualdisplay \\\n",
        "    PyOpenGL \\\n",
        "    PyOpenGL-accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f32781022b0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gymnasium as gym\n",
        "import gymnasium_robotics\n",
        "import torch\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda:0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yeo3s-QPnZH"
      },
      "source": [
        "#### Create the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lKGbkg_6uIGK"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity, her_probability=0.8):#her_probability will decided\n",
        "    #how often we will select samples of experience crated by hindsight experience replay\n",
        "    self.her_probability = her_probability\n",
        "    self.buffer = deque(maxlen=capacity//2)\n",
        "    self.her_buffer = deque(maxlen=capacity//2)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer) + len(self.her_buffer)\n",
        "\n",
        "  def append(self, experience, her=False):\n",
        "    if her:\n",
        "      self.her_buffer.append(experience)\n",
        "    else:\n",
        "      self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    her_batch_size = int(batch_size * self.her_probability)#we are calculating how many\n",
        "    #number of entries will be in her_buffer and how many in normal buffer\n",
        "    regular_batch_size = batch_size - her_batch_size\n",
        "\n",
        "    batch = random.sample(self.buffer, regular_batch_size)#this batch samples from\n",
        "    #normal buffer\n",
        "    her_batch = random.sample(self.her_buffer, her_batch_size)\n",
        "    #this batch samples from her_buffer\n",
        "    full_batch = list(batch + her_batch)\n",
        "    #at the end, we combine the two batches\n",
        "    random.shuffle(full_batch)#just to ensure that every batch of this buffer that our\n",
        "    #neural network takes as input contains both kind of entires\n",
        "    return full_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YtGko6LVQaJz"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihkyoL5WQgGm"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sShXogWB_o-N"
      },
      "outputs": [],
      "source": [
        "class RepeatActionWrapper(gym.Wrapper):\n",
        "  def __init__(self, env, n):\n",
        "    super().__init__(env)\n",
        "    self.env = env\n",
        "    self.n = n\n",
        "      \n",
        "  def step(self, action):\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    for _ in range(self.n):\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    return next_state, total_reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r6_QhEyBNSrM"
      },
      "outputs": [],
      "source": [
        "def create_environment(name):\n",
        "  env = gym.make(name, render_mode='rgb_array')\n",
        "  env = RecordVideo(env, video_folder='./videos4', episode_trigger=lambda x: x % 50 == 0)\n",
        "  # env = RepeatActionWrapper(env, n=4)\n",
        "  env = RecordEpisodeStatistics(env)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8fKGgFzQ4EX"
      },
      "source": [
        "#### Update the target network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-q-OJaPnBvKf"
      },
      "outputs": [],
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KipG5q0vAKi"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "V_l-HLfxtunM"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size + out_dims, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),           \n",
        "        nn.Linear(hidden_size, 1),\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    if isinstance(state, np.ndarray):\n",
        "      state = torch.from_numpy(state).to(device)\n",
        "    if isinstance(action, np.ndarray):\n",
        "      action = torch.from_numpy(action).to(device)\n",
        "    in_vector = torch.hstack((state, action))\n",
        "    return self.net(in_vector.float())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIkXBOhtug5p"
      },
      "source": [
        "#### Create the gradient policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "noMyzZ5DyOdQ"
      },
      "outputs": [],
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims, max):\n",
        "    super().__init__()\n",
        "\n",
        "    self.max = torch.from_numpy(max).to(device)\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.linear_mu = nn.Linear(hidden_size, out_dims)\n",
        "    # self.linear_std = nn.Linear(hidden_size, out_dims)\n",
        "    self.linear_log_std = nn.Linear(hidden_size, out_dims)\n",
        "\n",
        "  def forward(self, obs):\n",
        "    if isinstance(obs, np.ndarray):\n",
        "      obs = torch.from_numpy(obs).to(device)\n",
        "    x = self.net(obs.float())\n",
        "    mu = self.linear_mu(x)\n",
        "    # std = self.linear_std(x).clamp(0.01, 5.0)\n",
        "\n",
        "    log_std = self.linear_log_std(x)\n",
        "    log_std = log_std.clamp(-20, 2)\n",
        "    std = log_std.exp()\n",
        "\n",
        "    dist = Normal(mu, std)\n",
        "    action = dist.rsample()\n",
        "    log_prob = dist.log_prob(action)\n",
        "    log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
        "    log_prob -= (2* (np.log(2) - action - F.softplus(-2*action))).sum(dim=-1, keepdim=True)\n",
        "\n",
        "    action = torch.tanh(action) * self.max\n",
        "    return action, log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTZH7WwlvKQ8"
      },
      "source": [
        "#### Soft actor-critic algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GZcn2-X4yOfz"
      },
      "outputs": [],
      "source": [
        "class SACHER(LightningModule):\n",
        "\n",
        "  def __init__(self, env_name, capacity=100_000, batch_size=256, lr=1e-3, \n",
        "               hidden_size=256, gamma=0.99, loss_fn=F.smooth_l1_loss, optim=AdamW, \n",
        "               samples_per_epoch=1_000, tau=0.05, alpha=0.02, her=0.8):\n",
        "\n",
        "    super().__init__()\n",
        "    self.automatic_optimization = False  # Disable automatic optimization\n",
        "    self.env = create_environment(env_name)\n",
        "\n",
        "    ag_size = self.env.observation_space['achieved_goal'].shape[0]\n",
        "    dg_size = self.env.observation_space['desired_goal'].shape[0]\n",
        "    obs_size = self.env.observation_space['observation'].shape[0]\n",
        "\n",
        "    action_dims = self.env.action_space.shape[0]\n",
        "    max_action = self.env.action_space.high\n",
        "\n",
        "    self.q_net1 = DQN(hidden_size, obs_size + dg_size, action_dims)\n",
        "    self.q_net2 = DQN(hidden_size, obs_size + dg_size, action_dims)\n",
        "    self.policy = GradientPolicy(hidden_size, obs_size + dg_size, action_dims, max_action)\n",
        "\n",
        "    self.target_policy = copy.deepcopy(self.policy)\n",
        "    self.target_q_net1 = copy.deepcopy(self.q_net1)\n",
        "    self.target_q_net2 = copy.deepcopy(self.q_net2)\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity, her_probability=her)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch * 2:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episodes()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_episodes(self, policy=None):\n",
        "      state, _ = self.env.reset()\n",
        "      done = False\n",
        "      truncate = False\n",
        "      while not (done or truncate):\n",
        "        desired_state = np.hstack([state['observation'], state['desired_goal']])\n",
        "        achieved_state = np.hstack([state['observation'], state['achieved_goal']])\n",
        "        \n",
        "        if policy and random.random() > 0.1:\n",
        "          action, _ = self.policy(desired_state)\n",
        "          action = action.cpu().numpy()\n",
        "        else:\n",
        "          action = self.env.action_space.sample()\n",
        "          \n",
        "        next_state, reward, done, truncate, info = self.env.step(action)\n",
        "\n",
        "        next_desired_state = np.hstack([next_state['observation'], next_state['desired_goal']])\n",
        "        next_achieved_state = np.hstack([next_state['observation'], next_state['achieved_goal']])\n",
        "\n",
        "        # Desired goal.\n",
        "        exp = (desired_state, action, reward, done, next_desired_state)\n",
        "        self.buffer.append(exp)\n",
        "\n",
        "        # Achieved goal.\n",
        "        reward = self.env.compute_reward(next_state['achieved_goal'], next_state['achieved_goal'], info)\n",
        "        exp = (achieved_state, action, reward, done, next_achieved_state)\n",
        "        self.buffer.append(exp, her=True)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.policy(x)\n",
        "    return output\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    # q_net_parameters = itertools.chain(self.q_net1.parameters(), self.q_net2.parameters())\n",
        "    # q_net_optimizer = self.hparams.optim(q_net_parameters, lr=self.hparams.lr)\n",
        "    # policy_optimizer = self.hparams.optim(self.policy.parameters(), lr=self.hparams.lr)\n",
        "    # return [q_net_optimizer, policy_optimizer]\n",
        "    q_net1_optimizer = self.hparams.optim(self.q_net1.parameters(), lr=self.hparams.lr)\n",
        "    q_net2_optimizer = self.hparams.optim(self.q_net2.parameters(), lr=self.hparams.lr)\n",
        "    policy_optimizer = self.hparams.optim(self.policy.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net1_optimizer, q_net2_optimizer, policy_optimizer]\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "    opt_q_net1, opt_q_net2, opt_policy = self.optimizers()\n",
        "\n",
        "    # if optimizer_idx == 0:\n",
        "\n",
        "    action_values1 = self.q_net1(states, actions)\n",
        "    action_values2 = self.q_net2(states, actions)\n",
        "\n",
        "    target_actions, target_log_probs = self.target_policy(next_states)\n",
        "\n",
        "    next_action_values = torch.min(\n",
        "        self.target_q_net1(next_states, target_actions),\n",
        "        self.target_q_net2(next_states, target_actions)\n",
        "    )\n",
        "    next_action_values[dones] = 0.0\n",
        "\n",
        "    expected_action_values = rewards + self.hparams.gamma * (next_action_values - self.hparams.alpha * target_log_probs)\n",
        "\n",
        "    q_loss1 = self.hparams.loss_fn(action_values1, expected_action_values)\n",
        "    q_loss2 = self.hparams.loss_fn(action_values2, expected_action_values)\n",
        "    opt_q_net1.zero_grad()\n",
        "    q_loss1.backward(retain_graph=True)\n",
        "    opt_q_net1.step()\n",
        "    opt_q_net2.zero_grad()\n",
        "    q_loss2.backward()\n",
        "    opt_q_net2.step()\n",
        "    q_loss_total = q_loss1 + q_loss2\n",
        "    self.log(\"episode/Q-Loss\", q_loss_total)\n",
        "    \n",
        "    # q_loss_total = q_loss1 + q_loss2\n",
        "    # self.log(\"episode/Q-Loss\", q_loss_total)\n",
        "      # return q_loss_total\n",
        "\n",
        "    # elif optimizer_idx == 1:\n",
        "\n",
        "    actions, log_probs = self.policy(states)\n",
        "\n",
        "    action_values = torch.min(\n",
        "        self.q_net1(states, actions),\n",
        "        self.q_net2(states, actions)\n",
        "    )\n",
        "\n",
        "    policy_loss = (self.hparams.alpha * log_probs - action_values).mean()\n",
        "    opt_policy.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    opt_policy.step()\n",
        "    self.log(\"episode/Policy Loss\", policy_loss)\n",
        "      # return policy_loss\n",
        "\n",
        "  def on_train_epoch_end(self):\n",
        "    self.play_episodes(policy=self.policy)\n",
        "\n",
        "    polyak_average(self.q_net1, self.target_q_net1, tau=self.hparams.tau)\n",
        "    polyak_average(self.q_net2, self.target_q_net2, tau=self.hparams.tau)\n",
        "    polyak_average(self.policy, self.target_policy, tau=self.hparams.tau)\n",
        "\n",
        "    self.log(\"episode/episode_return\", self.env.return_queue[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "D_wtApvqyOh5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/lightning_logs/': No such file or directory\n",
            "rm: cannot remove '/content/videos/': No such file or directory\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ERROR: Failed to start `tensorboard`: [Errno 8] Exec format error:\n",
              "'tensorboard'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YNu1BWZBQGTg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/akhters/.local/lib/python3.8/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment FetchReach-v1 is out of date. You should consider upgrading to version `v2`.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/home/akhters/.local/lib/python3.8/site-packages/gymnasium_robotics/envs/robot_env.py:361: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
            "  logger.warn(\n",
            "/home/akhters/.local/lib/python3.8/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/home/akhters/.local/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_vector_env to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_vector_env` for environment variables or `env.get_wrapper_attr('is_vector_env')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/home/akhters/.local/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/home/akhters/.local/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.compute_reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.compute_reward` for environment variables or `env.get_wrapper_attr('compute_reward')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 samples in experience buffer. Filling...\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-0.mp4\n",
            "100 samples in experience buffer. Filling...\n",
            "200 samples in experience buffer. Filling...\n",
            "300 samples in experience buffer. Filling...\n",
            "400 samples in experience buffer. Filling...\n",
            "500 samples in experience buffer. Filling...\n",
            "600 samples in experience buffer. Filling...\n",
            "700 samples in experience buffer. Filling...\n",
            "800 samples in experience buffer. Filling...\n",
            "900 samples in experience buffer. Filling...\n",
            "1000 samples in experience buffer. Filling...\n",
            "1100 samples in experience buffer. Filling...\n",
            "1200 samples in experience buffer. Filling...\n",
            "1300 samples in experience buffer. Filling...\n",
            "1400 samples in experience buffer. Filling...\n",
            "1500 samples in experience buffer. Filling...\n",
            "1600 samples in experience buffer. Filling...\n",
            "1700 samples in experience buffer. Filling...\n",
            "1800 samples in experience buffer. Filling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name          | Type           | Params | Mode \n",
            "---------------------------------------------------------\n",
            "0 | q_net1        | DQN            | 70.7 K | train\n",
            "1 | q_net2        | DQN            | 70.7 K | train\n",
            "2 | policy        | GradientPolicy | 71.4 K | train\n",
            "3 | target_policy | GradientPolicy | 71.4 K | train\n",
            "4 | target_q_net1 | DQN            | 70.7 K | train\n",
            "5 | target_q_net2 | DQN            | 70.7 K | train\n",
            "---------------------------------------------------------\n",
            "425 K     Trainable params\n",
            "0         Non-trainable params\n",
            "425 K     Total params\n",
            "1.702     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1900 samples in experience buffer. Filling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/akhters/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff5ec746f75c451ab6b7daaadcd3bdbe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-50.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-50.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-50.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-100.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-100.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-100.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-150.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-150.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-150.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-200.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-200.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-200.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-250.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-250.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-250.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-300.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-300.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-300.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-350.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-350.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-350.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-400.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-400.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-400.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-450.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-450.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-450.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-500.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-500.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-500.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-550.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-550.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-550.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-600.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-600.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-600.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-650.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-650.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-650.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-700.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-700.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-700.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-750.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-750.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-750.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-800.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-800.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-800.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-850.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-850.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-850.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-900.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-900.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-900.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-950.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-950.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-950.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1000.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1000.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1050.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1050.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1050.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1100.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1100.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1100.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1150.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1150.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1150.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1200.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1200.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1200.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1250.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1250.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1250.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1300.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1300.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1300.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1350.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1350.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1350.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1400.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1400.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1400.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1450.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1450.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1450.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1500.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1500.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1500.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1550.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1550.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1550.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1600.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1600.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1600.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1650.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1650.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1650.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1700.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1700.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1700.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1750.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1750.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1750.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1800.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1800.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1800.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1850.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1850.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1850.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1900.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1900.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1900.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1950.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1950.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-1950.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2000.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2000.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2050.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2050.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2050.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2100.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2100.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2100.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2150.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2150.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2150.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2200.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2200.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2200.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2250.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2250.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2250.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2300.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2300.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2300.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2350.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2350.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2350.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2400.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2400.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2400.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2450.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2450.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2450.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2500.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2500.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2500.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2550.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2550.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2550.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2600.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2600.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2600.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2650.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2650.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2650.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2700.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2700.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2700.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2750.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2750.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2750.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2800.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2800.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2800.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2850.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2850.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2850.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2900.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2900.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2900.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2950.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2950.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-2950.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3000.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3000.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3050.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3050.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3050.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3100.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3100.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3100.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3150.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3150.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3150.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3200.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3200.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3200.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3250.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3250.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3250.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3300.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3300.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3300.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3350.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3350.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3350.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3400.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3400.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3400.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3450.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3450.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3450.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3500.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3500.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3500.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3550.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3550.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3550.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3600.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3600.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3600.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3650.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3650.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3650.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3700.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3700.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3700.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3750.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3750.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3750.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3800.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3800.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3800.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3850.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3850.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3850.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3900.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3900.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3900.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3950.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3950.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-3950.mp4\n",
            "Moviepy - Building video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-4000.mp4.\n",
            "Moviepy - Writing video /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-4000.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /home/akhters/Reinforcement Learning/advanced_rl_dqn_to_sac_complete-main/videos4/rl-video-episode-4000.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=4000` reached.\n"
          ]
        }
      ],
      "source": [
        "algo = SACHER('FetchReach-v1', lr=1e-3, alpha=0.2, tau=0.1)\n",
        "\n",
        "trainer = Trainer(\n",
        "    # gpus=num_gpus, \n",
        "    max_epochs=4_000,\n",
        "    log_every_n_steps=1\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pbok-MwMQGWZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
